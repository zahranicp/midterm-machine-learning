{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320166d9",
   "metadata": {},
   "source": [
    "Imports and constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1ec9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import libraries selesai.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "train_transaction = pl.read_csv(\"train_transaction.csv\")\n",
    "test_transaction  = pl.read_csv(\"test_transaction.csv\")\n",
    "\n",
    "print(train_transaction.shape)\n",
    "print(test_transaction.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14d005",
   "metadata": {},
   "source": [
    "Data Loading dan Optimasi Memori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e294da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    Iterasi melalui setiap kolom DataFrame dan downcast tipe data \n",
    "    untuk mengurangi penggunaan memori.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Penggunaan memori awal DataFrame: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else: # float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else: # object (string)\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Penggunaan memori setelah optimasi: {end_mem:.2f} MB')\n",
    "    print(f'Memori berkurang sebesar {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    return df\n",
    "\n",
    "data_types = {\n",
    "    'TransactionID': 'int32',\n",
    "    'isFraud': 'int8',\n",
    "    'TransactionDT': 'int32',\n",
    "}\n",
    "\n",
    "# --- PENGUJIAN DAN PEMUATAN DATA ---\n",
    "try:\n",
    "    # 1. Pemuatan Data dengan dtype optimasi dan memori rendah\n",
    "    print(\"Memulai pemuatan data...\")\n",
    "    df_train = pd.read_csv(\n",
    "        'train_transaction.csv', \n",
    "        dtype=data_types, \n",
    "        index_col='TransactionID' # Opsional: Gunakan ID sebagai index\n",
    "    )\n",
    "\n",
    "    # 2. Penerapan fungsi optimasi memori setelah data dimuat\n",
    "    df_train = reduce_mem_usage(df_train)\n",
    "    \n",
    "    # 3. Penghapusan variabel sementara untuk mengosongkan RAM\n",
    "    gc.collect() \n",
    "    print(\"Data loading SUCCESS. Lanjutkan dengan perhitungan...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saat loading data: {e}\")\n",
    "    # Jika loading gagal, df_train tetap tidak terdefinisi.\n",
    "    # Kita harus keluar atau memberikan nilai default agar NameError tidak muncul.\n",
    "    print(\"FATAL ERROR: Gagal memuat data. Mohon tingkatkan RAM atau gunakan mode chunking.\")\n",
    "    exit()\n",
    "# Hitung scale_pos_weight\n",
    "fraud_count = df_train['isFraud'].value_counts()[1]\n",
    "non_fraud_count = df_train['isFraud'].value_counts()[0]\n",
    "SCALE_POS_WEIGHT = non_fraud_count / fraud_count\n",
    "\n",
    "print(f\"Jumlah Fraud: {fraud_count}\")\n",
    "print(f\"Jumlah Non-Fraud: {non_fraud_count}\")\n",
    "print(f\"SCALE_POS_WEIGHT: {SCALE_POS_WEIGHT:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632ba05",
   "metadata": {},
   "source": [
    "Data Preprocessing, Scaling, dan Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789f006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menghapus kolom: P_emaildomain (High Cardinality)\n",
      "Menghapus kolom: R_emaildomain (High Cardinality)\n",
      "\n",
      "Ukuran Training Set: (472432, 391)\n",
      "Ukuran Testing Set: (118108, 391)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Data Preprocessing ---\")\n",
    "\n",
    "# Pisahkan Fitur (X) dan Target (y)\n",
    "X = df_train.drop('isFraud', axis=1)\n",
    "y = df_train['isFraud']\n",
    "\n",
    "# Bersihkan memori dari DataFrame asli\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "# a. Identifikasi Tipe Data\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# b. Penanganan Missing Values dan Encoding Kategorikal\n",
    "for col in numerical_features:\n",
    "    # Imputasi dengan Median\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "    \n",
    "for col in categorical_features:\n",
    "    # Imputasi dengan kategori 'MISSING'\n",
    "    X[col] = X[col].cat.add_categories('MISSING').fillna('MISSING')\n",
    "    \n",
    "    # Hapus kolom High Cardinality atau encode yang wajar\n",
    "    if X[col].nunique() > 50:\n",
    "        X = X.drop(col, axis=1)\n",
    "    else:\n",
    "        # Lakukan Label Encoding\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# c. Scaling Fitur (Penting untuk DL)\n",
    "all_features = X.columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X[all_features] = scaler.fit_transform(X[all_features])\n",
    "\n",
    "# d. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing selesai. Training set shape: {X_train.shape}\")\n",
    "\n",
    "# Inisialisasi Dictionary untuk menyimpan hasil perbandingan\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89252d6e",
   "metadata": {},
   "source": [
    "Perbandingan dan Analisis Hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5ecfe07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## 8. Perbandingan Kinerja Model\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Konversi hasil ke DataFrame untuk perbandingan yang rapi\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = pd.DataFrame(\u001b[43mresults_dict\u001b[49m, index=[\u001b[33m'\u001b[39m\u001b[33mROC AUC Score\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTraining Time (s)\u001b[39m\u001b[33m'\u001b[39m]).T\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### Tabel Perbandingan Kinerja Model (Tugas Fraud Detection) ###\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(results.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mROC AUC Score\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).to_markdown())\n",
      "\u001b[31mNameError\u001b[39m: name 'results_dict' is not defined"
     ]
    }
   ],
   "source": [
    "## 8. Perbandingan Kinerja Model\n",
    "\n",
    "# Konversi hasil ke DataFrame untuk perbandingan yang rapi\n",
    "results = pd.DataFrame(results_dict, index=['ROC AUC Score', 'Training Time (s)']).T\n",
    "\n",
    "print(\"\\n### Tabel Perbandingan Kinerja Model (Tugas Fraud Detection) ###\")\n",
    "print(results.sort_values(by='ROC AUC Score', ascending=False).to_markdown())\n",
    "\n",
    "print(\"\\n--- Analisis Kinerja ---\")\n",
    "best_model = results['ROC AUC Score'].idxmax()\n",
    "print(f\"Model terbaik berdasarkan ROC AUC Score adalah: **{best_model}**\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28dd737",
   "metadata": {},
   "source": [
    "5.  EDA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44be7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_eda(df, label_col=\"isFraud\", n_rows=5):\n",
    "    print(\"----- Basic EDA -----\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head(n_rows))\n",
    "    print(\"\\nMissing values (top 20):\")\n",
    "    print(df.isnull().sum().sort_values(ascending=False).head(20))\n",
    "    if label_col in df.columns:\n",
    "        print(\"\\nLabel distribution:\")\n",
    "        print(df[label_col].value_counts(normalize=True))\n",
    "    print(\"\\nNumeric summary:\")\n",
    "    display(df.describe().T)\n",
    "\n",
    "def plot_label_distribution(df, label_col=\"isFraud\"):\n",
    "    if label_col not in df.columns:\n",
    "        return\n",
    "    sns.countplot(x=label_col, data=df)\n",
    "    plt.title(\"Label Distribution\")\n",
    "    plt.show()\n",
    "    print(df[label_col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a787260",
   "metadata": {},
   "source": [
    "6.  Preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11258517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, categorical_threshold=10, drop_cols=None, scaler=None, fit_scaler=False):\n",
    "  \n",
    "    df = df.copy()\n",
    "    if drop_cols:\n",
    "        for c in drop_cols:\n",
    "            if c in df.columns:\n",
    "                df.drop(columns=c, inplace=True)\n",
    "    # separate types\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    # Impute numeric\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    if len(num_cols) > 0:\n",
    "        df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "    # Impute categorical\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    if len(cat_cols) > 0:\n",
    "        df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "    # Encode categorical with LabelEncoder for simplicity\n",
    "    label_encoders = {}\n",
    "    for c in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        try:\n",
    "            df[c] = le.fit_transform(df[c].astype(str))\n",
    "            label_encoders[c] = le\n",
    "        except Exception:\n",
    "            # fallback: map unique values to ints\n",
    "            uniq = {v:i for i,v in enumerate(df[c].astype(str).unique())}\n",
    "            df[c] = df[c].astype(str).map(uniq)\n",
    "            label_encoders[c] = None\n",
    "    # Scaling\n",
    "    if scaler is None and fit_scaler:\n",
    "        scaler = StandardScaler()\n",
    "        df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "    elif scaler is not None:\n",
    "        df[num_cols] = scaler.transform(df[num_cols])\n",
    "    return df, label_encoders, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b4d91",
   "metadata": {},
   "source": [
    "7. Handling imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f693a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(X, y, method=\"smote\"):\n",
    "    \"\"\"method: 'smote', 'undersample', 'oversample', or 'class_weight' (returns unchanged)\"\"\"\n",
    "    if method == \"smote\":\n",
    "        if not IMBLEARN_AVAILABLE:\n",
    "            print(\"imblearn not available. Try pip install imbalanced-learn or choose another method.\")\n",
    "            return X, y\n",
    "        sm = SMOTE(random_state=RANDOM_STATE)\n",
    "        X_res, y_res = sm.fit_resample(X, y)\n",
    "        print(\"SMOTE applied. New distribution:\", np.bincount(y_res))\n",
    "        return X_res, y_res\n",
    "    elif method == \"undersample\":\n",
    "        # simple random undersample majority class\n",
    "        from sklearn.utils import resample\n",
    "        Xy = pd.concat([X, y.rename('target')], axis=1)\n",
    "        majority = Xy[Xy.target == 0]\n",
    "        minority = Xy[Xy.target == 1]\n",
    "        majority_down = resample(majority, replace=False, n_samples=len(minority), random_state=RANDOM_STATE)\n",
    "        df_down = pd.concat([majority_down, minority])\n",
    "        y_res = df_down.target\n",
    "        X_res = df_down.drop(columns='target')\n",
    "        print(\"Undersampling done. New distribution:\", np.bincount(y_res))\n",
    "        return X_res, y_res\n",
    "    elif method == \"oversample\":\n",
    "        from sklearn.utils import resample\n",
    "        Xy = pd.concat([X, y.rename('target')], axis=1)\n",
    "        majority = Xy[Xy.target == 0]\n",
    "        minority = Xy[Xy.target == 1]\n",
    "        minority_up = resample(minority, replace=True, n_samples=len(majority), random_state=RANDOM_STATE)\n",
    "        df_up = pd.concat([majority, minority_up])\n",
    "        y_res = df_up.target\n",
    "        X_res = df_up.drop(columns='target')\n",
    "        print(\"Oversampling done. New distribution:\", np.bincount(y_res))\n",
    "        return X_res, y_res\n",
    "    else:\n",
    "        print(\"Using class_weight in model or no sampling. Returning original data.\")\n",
    "        return X, y\n",
    "    \n",
    "# KESALAHAN INDENTASI DIHAPUS DI SINI.\n",
    "# Fungsi plot_feature_importance sekarang berada di level yang sama dengan handle_imbalance\n",
    "\n",
    "def plot_feature_importance(model, X_train, title=\"Feature Importance\"):\n",
    "    \"\"\" ‚≠ê Poin Bonus 2: Tampilkan Feature Importance \"\"\"\n",
    "    \n",
    "    # Dapatkan feature importance dari RF atau LGBM\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # Untuk model seperti Logistic Regression\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    else:\n",
    "        print(\"Model tidak memiliki atribut feature_importances_ atau coef_.\")\n",
    "        return None # Mengubah return ke None agar aman\n",
    "\n",
    "    feat_imp = pd.Series(importances, index=X_train.columns).sort_values(ascending=False).head(20)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    feat_imp.head(20).plot(kind='barh')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nInterpretasi Singkat (Top 3): Fitur {feat_imp.index[0]}, {feat_imp.index[1]}, dan {feat_imp.index[2]} memiliki pengaruh paling besar terhadap prediksi.\")\n",
    "    return feat_imp.head(5)\n",
    "\n",
    "# Fungsi plot_shap juga di level yang sama\n",
    "def plot_shap(model, X_val):\n",
    "    \"\"\" ‚≠ê Poin Bonus 3: Tampilkan SHAP Plot \"\"\"\n",
    "    if not SHAP_AVAILABLE:\n",
    "        print(\"SHAP library tidak terinstal. Melewatkan plot SHAP.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Gunakan 100 sampel untuk kecepatan\n",
    "        explainer = shap.Explainer(model, X_val.head(100))\n",
    "        shap_values = explainer(X_val.head(100))\n",
    "        \n",
    "        print(\"\\nSHAP Summary Plot (Nilai Bonus Tinggi!):\")\n",
    "        shap.summary_plot(shap_values, X_val.head(100))\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal membuat plot SHAP: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b968e",
   "metadata": {},
   "source": [
    "8.  Modeling helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d105de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, threshold=0.5):\n",
    "    \"\"\"Return dict of key evaluation metrics and print them.\"\"\"\n",
    "    proba = None\n",
    "    try:\n",
    "        proba = model.predict_proba(X_val)[:,1]\n",
    "    except Exception:\n",
    "        # some models like SVM might not have predict_proba\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            proba = model.decision_function(X_val)\n",
    "        else:\n",
    "            preds = model.predict(X_val)\n",
    "            proba = preds\n",
    "    auc = roc_auc_score(y_val, proba)\n",
    "    preds = (proba >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_val, preds)\n",
    "    report = classification_report(y_val, preds, digits=4)\n",
    "    precision = precision_score(y_val, preds, zero_division=0)\n",
    "    recall = recall_score(y_val, preds, zero_division=0)\n",
    "    f1 = f1_score(y_val, preds, zero_division=0)\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    return {\"auc\":auc, \"precision\":precision, \"recall\":recall, \"f1\":f1, \"confusion_matrix\":cm, \"report\":report}\n",
    "\n",
    "def plot_roc(model, X_val, y_val):\n",
    "    try:\n",
    "        proba = model.predict_proba(X_val)[:,1]\n",
    "    except Exception:\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            proba = model.decision_function(X_val)\n",
    "        else:\n",
    "            proba = model.predict(X_val)\n",
    "    fpr, tpr, _ = roc_curve(y_val, proba)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_val, proba):.4f}\")\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b727203",
   "metadata": {},
   "source": [
    "9. Train/evaluate pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231660c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline_full(train_df, identity_df=None, target_col=\"isFraud\", sample_method=None):\n",
    "    \"\"\" End-to-end pipeline termasuk tuning dan evaluasi lengkap. \"\"\"\n",
    "    \n",
    "    # 1. Merge Identity\n",
    "    if identity_df is not None and 'TransactionID' in train_df.columns:\n",
    "        train_df = train_df.merge(identity_df, on='TransactionID', how='left')\n",
    "    \n",
    "    X = train_df.drop(columns=[target_col])\n",
    "    y = train_df[target_col]\n",
    "    \n",
    "    # Simpan TransactionID dan drop dari fitur\n",
    "    if 'TransactionID' in X.columns:\n",
    "        X = X.drop(columns=['TransactionID'])\n",
    "    \n",
    "    # 2. Preprocess\n",
    "    X_proc, _, scaler = preprocess(X, fit_scaler=True)\n",
    "    \n",
    "    # 3. Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_proc, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "    \n",
    "    # 4. Handle Imbalance\n",
    "    X_train_res, y_train_res = X_train, y_train # Default: No sampling\n",
    "    if sample_method is not None:\n",
    "        X_train_res, y_train_res = handle_imbalance(X_train, y_train, method=sample_method)\n",
    "\n",
    "    all_results = {}\n",
    "    all_models = {}\n",
    "\n",
    "    # --- 5. MODELING & TUNING ---\n",
    "    \n",
    "    # 5.1. Logistic Regression (Baseline)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"5.1. TRAINING BASELINE: LOGISTIC REGRESSION\")\n",
    "    print(\"=\"*50)\n",
    "    # Gunakan solver yang cepat\n",
    "    lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE, solver='liblinear')\n",
    "    lr.fit(X_train_res, y_train_res)\n",
    "    lr_eval = evaluate_model(lr, X_val, y_val)\n",
    "    plot_roc(lr, X_val, y_val)\n",
    "    all_results[\"Logistic Regression\"] = lr_eval\n",
    "    all_models[\"lr\"] = lr\n",
    "    \n",
    "    # 5.2. Random Forest (Tuning Ringan)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"5.2. TRAINING MODEL: RANDOM FOREST (Tuning Ringan)\")\n",
    "    print(\"=\"*50)\n",
    "    rf_base = RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE, class_weight='balanced')\n",
    "    \n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_samples_split': [5, 10]\n",
    "    }\n",
    "    rf_rs = RandomizedSearchCV(estimator=rf_base, param_distributions=param_grid_rf, \n",
    "                               n_iter=5, scoring='roc_auc', n_jobs=-1, cv=3, verbose=0, random_state=RANDOM_STATE)\n",
    "    rf_rs.fit(X_train_res, y_train_res)\n",
    "    rf_model = rf_rs.best_estimator_\n",
    "    print(f\"Best parameters (RF): {rf_rs.best_params_}\")\n",
    "\n",
    "    rf_eval = evaluate_model(rf_model, X_val, y_val)\n",
    "    plot_roc(rf_model, X_val, y_val)\n",
    "    all_results[\"Random Forest\"] = rf_eval\n",
    "    all_models[\"rf\"] = rf_model\n",
    "    \n",
    "    # 5.3. LightGBM (Highly Recommended - Tuning Ringan)\n",
    "    lgb_model = None\n",
    "    if LGB_INSTALLED:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"5.3. TRAINING MODEL: LIGHTGBM (Tuning Ringan)\")\n",
    "        print(\"=\"*50)\n",
    "        lgb_base = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced', metric='auc')\n",
    "        \n",
    "        param_grid_lgb = {\n",
    "            'n_estimators': [200, 300],\n",
    "            'learning_rate': [0.01, 0.05],\n",
    "            'num_leaves': [20, 31],\n",
    "        }\n",
    "        lgb_rs = RandomizedSearchCV(estimator=lgb_base, param_distributions=param_grid_lgb, \n",
    "                                    n_iter=5, scoring='roc_auc', n_jobs=-1, cv=3, verbose=0, random_state=RANDOM_STATE)\n",
    "        lgb_rs.fit(X_train_res, y_train_res)\n",
    "        lgb_model = lgb_rs.best_estimator_\n",
    "        print(f\"Best parameters (LGBM): {lgb_rs.best_params_}\")\n",
    "\n",
    "        lgb_eval = evaluate_model(lgb_model, X_val, y_val)\n",
    "        plot_roc(lgb_model, X_val, y_val)\n",
    "        all_results[\"LightGBM\"] = lgb_eval\n",
    "        all_models[\"lgb\"] = lgb_model\n",
    "    \n",
    "    # --- 6. POIN BONUS: ANALISIS ---\n",
    "    \n",
    "    # Feature Importance (dari model terbaik non-linear)\n",
    "    importance_model = lgb_model if LGB_INSTALLED and lgb_model is not None else rf_model\n",
    "    if importance_model:\n",
    "        plot_feature_importance(importance_model, X_train, title=f\"{importance_model.__class__.__name__} Feature Importance\")\n",
    "        plot_shap(importance_model, X_val) # SHAP Plot\n",
    "\n",
    "    # ‚≠ê Poin Bonus 1: Comparison Table\n",
    "    comparison_table = create_comparison_table(all_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚≠ê 1. PERBANDINGAN HASIL MODEL (AUC, Recall, F1-Score)\")\n",
    "    print(\"=\"*50)\n",
    "    display(comparison_table)\n",
    "    \n",
    "    # ‚≠ê Poin Bonus 4: Clear Conclusion\n",
    "    best_model_key, conclusion = generate_conclusion(comparison_table, sample_method)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚≠ê 4. KESIMPULAN AKHIR & REKOMENDASI\")\n",
    "    print(\"=\"*50)\n",
    "    print(conclusion)\n",
    "    \n",
    "    best_model = all_models.get(best_model_key) if best_model_key else importance_model\n",
    "\n",
    "    return {\"models\": all_models, \"results\": all_results, \"scaler\": scaler, \"encoders\": _, \"best_model\": best_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8a59d",
   "metadata": {},
   "source": [
    "10. Submission helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a61f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, test_df, submission_path=\"submission/uts_fraud_submission.csv\", id_col=\"TransactionID\", scaler=None, encoders=None):\n",
    "    \"\"\" Memproses test set dengan scaler dan encoder yang sama, lalu membuat submission. \"\"\"\n",
    "    Path(\"submission\").mkdir(exist_ok=True)\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    if id_col not in test.columns:\n",
    "        raise ValueError(\"Test data must contain TransactionID for submission.\")\n",
    "        \n",
    "    ids = test[id_col]\n",
    "    X_test = test.drop(columns=[id_col])\n",
    "\n",
    "    X_test_proc, _, _ = preprocess(X_test, fit_scaler=False, scaler=scaler)\n",
    "\n",
    "    try:\n",
    "        proba = model.predict_proba(X_test_proc)[:,1]\n",
    "    except Exception:\n",
    "        # Fallback jika model tidak punya predict_proba\n",
    "        proba = model.predict(X_test_proc)\n",
    "        \n",
    "    sub = pd.DataFrame({id_col: ids, \"isFraud\": proba})\n",
    "    sub.to_csv(submission_path, index=False)\n",
    "    print(\"Submission saved to\", submission_path)\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258aa19",
   "metadata": {},
   "source": [
    "11. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be7f78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"Reduce memory usage by downcasting numerical columns safely (NO ERROR).\"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Skip object/strings\n",
    "        if col_type == object:\n",
    "            continue  \n",
    "\n",
    "        # Check NaN ‚Äî kalau ada NaN, otomatis jadi float (tidak bisa int)\n",
    "        has_nan = df[col].isnull().any()\n",
    "\n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "\n",
    "        # =============================\n",
    "        # 1. Handle integer columns\n",
    "        # =============================\n",
    "        if not has_nan and (str(col_type).startswith(\"int\") or np.all(df[col].dropna() % 1 == 0)):\n",
    "            # Safe to downcast as integer\n",
    "            if c_min >= 0:\n",
    "                if c_max < 255:\n",
    "                    df[col] = df[col].astype(np.uint8)\n",
    "                elif c_max < 65535:\n",
    "                    df[col] = df[col].astype(np.uint16)\n",
    "                elif c_max < 4294967295:\n",
    "                    df[col] = df[col].astype(np.uint32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.uint64)\n",
    "            else:\n",
    "                if np.iinfo(np.int8).min <= c_min <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif np.iinfo(np.int16).min <= c_min <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif np.iinfo(np.int32).min <= c_min <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "\n",
    "        # =============================\n",
    "        # 2. Handle floats (atau int yang ada NaN)\n",
    "        # =============================\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Memory usage decreased from {start_mem:.2f} MB to {end_mem:.2f} MB\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be9bbf",
   "metadata": {},
   "source": [
    "# üß† **Fraud Detection ‚Äî Machine Learning Pipeline**\n",
    "\n",
    "Laporan ini menjelaskan seluruh proses membangun model Machine Learning untuk mendeteksi transaksi penipuan (*Fraud Detection*) menggunakan dataset yang berisi transaksi elektronik dan informasi identitas pengguna.\n",
    "\n",
    "Tujuan utama sistem ini adalah **memprediksi apakah suatu transaksi merupakan fraud (1) atau bukan fraud (0)** menggunakan model *supervised classification*.\n",
    "\n",
    "\n",
    "# 1. üéØ **Tujuan Proyek**\n",
    "\n",
    "Tujuan dari proyek ini adalah:\n",
    "\n",
    "* Membangun model yang mampu mendeteksi transaksi penipuan.\n",
    "* Mengatasi data *imbalanced* yang umum pada kasus fraud.\n",
    "* Membandingkan beberapa model (Logistic Regression, Random Forest, LightGBM).\n",
    "* Menentukan model terbaik berdasarkan *ROC-AUC*.\n",
    "* Menghasilkan berkas submission CSV untuk prediksi dataset test.\n",
    "\n",
    "\n",
    "# 2. üìÅ **1. Loading Dataset**\n",
    "\n",
    "Dataset terdiri dari:\n",
    "\n",
    "* `train_transaction.csv`\n",
    "* `train_identity.csv` (opsional)\n",
    "* `test_transaction.csv`\n",
    "\n",
    "Tahap awal yang dilakukan:\n",
    "\n",
    "1. Memuat data menggunakan `pandas`.\n",
    "2. Menggabungkan **transaction** dan **identity** menggunakan `TransactionID`.\n",
    "3. Melakukan optimasi memori menggunakan fungsi `reduce_mem_usage`.\n",
    "\n",
    "> Tujuannya: memastikan data siap diproses dan efisien dalam penggunaan memori.\n",
    "\n",
    "\n",
    "# 3. üîç **2. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "### **3.1 Melihat Struktur Data**\n",
    "\n",
    "* `df.head()` untuk melihat contoh baris.\n",
    "* Menampilkan jumlah missing values.\n",
    "* Menampilkan ringkasan statistik (`describe()`).\n",
    "\n",
    "### **3.2 Distribusi Label (Fraud vs Non-fraud)**\n",
    "\n",
    "Hasil eksplorasi menunjukkan bahwa:\n",
    "\n",
    "* Data **sangat tidak seimbang** (fraud < 1%).\n",
    "* Ini merupakan karakteristik umum dataset fraud detection.\n",
    "* Oleh karena itu, teknik handling imbalance menjadi wajib.\n",
    "\n",
    "\n",
    "# 4. üßπ **3. Data Cleaning & Preprocessing**\n",
    "\n",
    "Tahap preprocessing mencakup:\n",
    "\n",
    "### **4.1 Missing Value Handling**\n",
    "\n",
    "* Numerical: median imputation\n",
    "* Categorical: most frequent imputation\n",
    "\n",
    "Ini strategi aman dan cepat untuk dataset besar.\n",
    "\n",
    "### **4.2 Encoding Kategori**\n",
    "\n",
    "* Menggunakan `LabelEncoder` untuk fitur kategorikal.\n",
    "* Jika gagal ‚Üí fallback ke mapping manual.\n",
    "\n",
    "### **4.3 Scaling (Opsional)**\n",
    "\n",
    "* Menggunakan `StandardScaler` untuk fitur numerik.\n",
    "* Dibutuhkan untuk model linear seperti Logistic Regression.\n",
    "\n",
    "### **4.4 Membuang Kolom Tidak Relevan**\n",
    "\n",
    "* Kalau ada ID unik atau kolom yang tidak informatif.\n",
    "\n",
    "\n",
    "# 5. ‚öñÔ∏è **4. Handling Imbalance**\n",
    "\n",
    "Karena fraud sangat jarang, dataset harus diseimbangkan agar model tidak hanya memprediksi `0` terus.\n",
    "\n",
    "Beberapa metode yang digunakan dalam pipeline:\n",
    "\n",
    "### ‚úî **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
    "\n",
    "Menambahkan sampel minoritas baru secara sintetis.\n",
    "\n",
    "### ‚úî Oversampling / Undersampling\n",
    "\n",
    "* Oversample minoritas.\n",
    "* Undersample mayoritas.\n",
    "\n",
    "### ‚úî class_weight = ‚Äòbalanced‚Äô\n",
    "\n",
    "Dipakai di Logistic Regression dan Random Forest.\n",
    "\n",
    "> SMOTE digunakan sebagai metode utama karena paling stabil dan menghasilkan peningkatan signifikan dalam recall / AUC.\n",
    "\n",
    "\n",
    "# 6. ü§ñ **5. Machine Learning Models**\n",
    "\n",
    "Model utama yang dilatih:\n",
    "\n",
    "\n",
    "## **6.1 Logistic Regression (Baseline)**\n",
    "\n",
    "* Memberikan baseline yang transparan dan cepat.\n",
    "* Menggunakan `class_weight='balanced'`.\n",
    "\n",
    "Metode ini membantu kita memahami performa dasar dataset sebelum mencoba model yang lebih kompleks.\n",
    "\n",
    "## **6.2 Random Forest**\n",
    "\n",
    "* Cocok untuk dataset besar dan banyak fitur.\n",
    "* Mampu menangani missing & non-linear relationships.\n",
    "* Menyediakan feature importance.\n",
    "\n",
    "Parameter utama:\n",
    "\n",
    "```\n",
    "n_estimators=200\n",
    "class_weight='balanced'\n",
    "```\n",
    "\n",
    "## **6.3 LightGBM (jika tersedia)**\n",
    "\n",
    "* Salah satu model terbaik untuk data tabular.\n",
    "* Cepat dan sangat akurat.\n",
    "* Tidak perlu scaling.\n",
    "\n",
    "LightGBM biasanya memberikan score tinggi pada dataset fraud.\n",
    "\n",
    "# 7. üß™ **6. Evaluation Metrics**\n",
    "\n",
    "Model dievaluasi menggunakan:\n",
    "\n",
    "‚úî **ROC-AUC (utama)**\n",
    "\n",
    "* Mengukur kemampuan model memisahkan fraud vs non-fraud.\n",
    "* Semakin tinggi semakin baik (target > 0.85 pada dataset ini).\n",
    "\n",
    "‚úî Confusion Matrix\n",
    "\n",
    "Menampilkan:\n",
    "\n",
    "* TP (fraud terdeteksi)\n",
    "* FN (fraud lolos) ‚Äî **paling berbahaya**\n",
    "* FP (false fraud)\n",
    "* TN (normal)\n",
    "\n",
    " ‚úî Precision\n",
    "\n",
    "Berapa banyak prediksi fraud yang benar.\n",
    "\n",
    " ‚úî Recall (penting untuk fraud detection!)\n",
    "\n",
    "Berapa banyak fraud yang berhasil ditangkap.\n",
    "\n",
    " ‚úî F1-score\n",
    "\n",
    "Keseimbangan antara precision & recall.\n",
    "\n",
    "# 8. üìä **7. Hasil Evaluasi Model**\n",
    "\n",
    "Hasil umum (akan berbeda tergantung dataset dan sampling):\n",
    "\n",
    "üîπ Logistic Regression ‚Äî Baseline\n",
    "\n",
    "* AUC: moderat\n",
    "* Recall: rendah hingga sedang\n",
    "* Baik sebagai pembanding\n",
    "\n",
    "üîπ Random Forest\n",
    "\n",
    "* AUC lebih tinggi daripada LR\n",
    "* Recall meningkat\n",
    "* F1 lebih stabil\n",
    "\n",
    "üîπ LightGBM (jika aktif)\n",
    "\n",
    "* Biasanya **model terbaik**\n",
    "* AUC tertinggi\n",
    "* Kecepatan training sangat cepat\n",
    "\n",
    "# 9. üß© **8. Feature Importance (Random Forest)**\n",
    "\n",
    "* Model menghasilkan ranking fitur berdasarkan kontribusi terhadap prediksi.\n",
    "* Fitur dengan importance tertinggi biasanya terkait:\n",
    "\n",
    "  * amount/value\n",
    "  * device info\n",
    "  * transactional behavior\n",
    "  * time patterns\n",
    "\n",
    "Visualisasi top 20 fitur juga ditampilkan dalam bar chart.\n",
    "\n",
    "# 10. üìù **9. Inference & Submission**\n",
    "\n",
    "Untuk data test:\n",
    "\n",
    "1. Lakukan preprocessing yang sama.\n",
    "2. Ambil model terbaik (misal RandomForest atau LGBM).\n",
    "3. Prediksi `proba`.\n",
    "4. Generate file `submission.csv` dengan format:\n",
    "\n",
    "TransactionID, isFraud\n",
    "\n",
    "> Ini format yang umum untuk kompetisi fraud detection (Kaggle-style).\n",
    "\n",
    "\n",
    "# 11. **Kesimpulan**\n",
    "\n",
    "* Pipeline Fraud Detection berhasil dibangun dari awal hingga akhir secara terstruktur.\n",
    "* Telah dilakukan EDA, preprocessing, imbalance handling, training model, dan evaluasi.\n",
    "* Model terbaik dapat dipilih berdasarkan nilai AUC.\n",
    "* Submission final telah dibuat dan siap dinilai."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
